{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Atención desnuda.\n",
        "\n",
        "El mecanismo de atención paso a paso\n",
        "\n",
        "Notebook original: Jared Ostmeyer"
      ],
      "metadata": {
        "id": "PlmTFPTVsCft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jrWTSTZtRMi",
        "outputId": "768c3866-5c44-4a20-b31f-4d4f6f0b4d8f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u6kl2i0Dr8pM"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pipeline:"
      ],
      "metadata": {
        "id": "Im9nqNPYsm7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##########################################################################################\n",
        "# Carga datos del MNIST\n",
        "##########################################################################################\n",
        "\n",
        "# Cargar datos de entrenamiento, validación y test del... MNIST!\n",
        "#\n",
        "def load_mnist(seed=None, device=torch.device('cpu')):\n",
        "\n",
        "  # Usamos random? Cuando llamamos decimos que sí.\n",
        "  #\n",
        "  generator = torch.Generator(device=device)\n",
        "  if seed is not None:\n",
        "    generator.manual_seed(seed)\n",
        "\n",
        "  # Cargamos el dataset MNIST\n",
        "  #\n",
        "  samples_train = torchvision.datasets.MNIST('./', train=True, download=True)\n",
        "  samples_test = torchvision.datasets.MNIST('./', train=False, download=True)\n",
        "\n",
        "  # Acomodar atributos y categorías\n",
        "  #\n",
        "  xs = samples_train.data.to(device)\n",
        "  num = xs.shape[0]\n",
        "  xs = xs.reshape([ num, 28**2, 1 ])\n",
        "  xs = xs.type(torch.float32)\n",
        "  ys = samples_train.train_labels.to(device)\n",
        "\n",
        "  xs_test = samples_test.data.to(device)\n",
        "  num_test = xs_test.shape[0]\n",
        "  xs_test = xs_test.reshape([ num_test, 28**2, 1 ])\n",
        "  xs_test = xs_test.type(torch.float32)\n",
        "  ys_test = samples_test.test_labels.to(device)\n",
        "\n",
        "  # train/valid split\n",
        "  #\n",
        "  num_train = int(num*5/6)\n",
        "  num_val = num-num_train\n",
        "\n",
        "  js = torch.randperm(num, generator=generator)\n",
        "  js_train = js[:num_train]\n",
        "  js_val = js[num_train:]\n",
        "\n",
        "  xs_train = xs[js_train]\n",
        "  ys_train = ys[js_train]\n",
        "\n",
        "  xs_val = xs[js_val]\n",
        "  ys_val = ys[js_val]\n",
        "\n",
        "  # Normalizamos los atributos\n",
        "  # Atención: Vamos a usar media y desvío -puntaje Z-\n",
        "  # No vamos a reescalar [0-1]\n",
        "\n",
        "  mean = torch.mean(xs_train, axis=0, keepdim=True)\n",
        "  variance = torch.var(xs_train, axis=0, keepdim=True)\n",
        "\n",
        "  xs_train = (xs_train-mean)/torch.std(variance+1.0E-8)\n",
        "  xs_val = (xs_val-mean)/torch.std(variance+1.0E-8)\n",
        "  xs_test = (xs_test-mean)/torch.std(variance+1.0E-8)\n",
        "\n",
        "  return xs_train, ys_train, xs_val, ys_val, xs_test, ys_test"
      ],
      "metadata": {
        "id": "TKFU5FuWsose"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################################################\n",
        "# Modelo\n",
        "##########################################################################################\n",
        "\n",
        "class SelfAttentionModel(torch.nn.Module):\n",
        "  def __init__(self, num_steps, num_channels, num_outputs, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    # Inicializamos las componentes de self-attention\n",
        "    # Cada peso valdrá entre -1/num_channels**0.5, 1/num_channels**0.5\n",
        "    self.K = torch.nn.Parameter((2.0*torch.rand(num_channels, num_channels)-1.0)/num_channels**0.5)\n",
        "    self.Q = torch.nn.Parameter((2.0*torch.rand(num_channels, num_channels)-1.0)/num_channels**0.5)\n",
        "    self.V = torch.nn.Parameter((2.0*torch.rand(num_channels, num_channels)-1.0)/num_channels**0.5)\n",
        "\n",
        "    self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    # Inicializamos capa de salida\n",
        "    #\n",
        "    self.out = torch.nn.Linear(num_steps*num_channels, num_outputs)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch_size, num_steps, num_channels = x.shape\n",
        "\n",
        "    # Definimos self-attention\n",
        "    #\n",
        "    y = []\n",
        "    for i in range(batch_size): # Dentro del batch, vamos de a 1. Nada de paralelizar.\n",
        "\n",
        "      x_i = x[i,:,:] # x_i shape = [ num_steps, num_channels ]\n",
        "\n",
        "      x_k_i = torch.matmul(x_i, self.K) # x_k_i shape = [ num_steps, num_channels ]\n",
        "      x_q_i = torch.matmul(x_i, self.Q) # x_q_i shape = [ num_steps, num_channels ]\n",
        "      x_v_i = torch.matmul(x_i, self.V) # x_v_i shape = [ num_steps, num_channels ]\n",
        "\n",
        "      w_i = self.softmax(torch.matmul(x_q_i, x_k_i.T)/num_channels**0.5) # w_i shape = [ num_steps, num_steps ]\n",
        "      y_i = torch.matmul(w_i, x_v_i) # y_i shape = [ num_steps, num_channels ]\n",
        "\n",
        "      y.append(y_i)\n",
        "    y = torch.stack(y, axis=0) # y shape = [ batch_size, num_steps, num_channels ]\n",
        "\n",
        "    # Flatten output\n",
        "    #\n",
        "    y_flat = y.reshape([ batch_size, num_steps*num_channels ]) # y_flat shape = [ batch_size, num_steps*num_channels ]\n",
        "\n",
        "    # Capa de salida\n",
        "    #\n",
        "    l = self.out(y_flat) # l shape = [ batch_size, num_outputs ]\n",
        "\n",
        "    return l\n",
        "\n",
        "##########################################################################################\n",
        "# Crear instancia de modelo, métricas y optimizador\n",
        "##########################################################################################\n",
        "\n",
        "model = SelfAttentionModel(num_steps=28**2, num_channels=1, num_outputs=10)\n",
        "probability = torch.nn.Softmax(dim=1)\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "accuracy = torchmetrics.classification.MulticlassAccuracy(num_classes=10)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "##########################################################################################\n",
        "# Sampleamos dataset a tensores\n",
        "##########################################################################################\n",
        "\n",
        "xs_train, ys_train, xs_val, ys_val, xs_test, ys_test = load_mnist(seed=42)\n",
        "\n",
        "dataset_train = torch.utils.data.TensorDataset(xs_train, ys_train)\n",
        "sampler_train = torch.utils.data.RandomSampler(dataset_train, replacement=True)\n",
        "loader_train = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=16, sampler=sampler_train, drop_last=True)\n",
        "\n",
        "##########################################################################################\n",
        "# Modelo\n",
        "##########################################################################################\n",
        "\n",
        "i_better = -1\n",
        "e_better = 1.0e8\n",
        "a_better = 0.0\n",
        "state_better = {}\n",
        "\n",
        "# Loopeamos el dataset por cuántas épocas?\n",
        "#\n",
        "for i in range(128):\n",
        "\n",
        "  # Entrenamos\n",
        "  #\n",
        "  model.train()\n",
        "  e_train = 0.0\n",
        "  a_train = 0.0\n",
        "  for xs_batch, ys_batch in iter(loader_train): # Must use `iter` or `enumerate` for efficiency\n",
        "    ls_batch = model(xs_batch)\n",
        "    ps_batch = probability(ls_batch) # De logit (negativo de la derivada de la función de entropía) a probabilidades\n",
        "    e_batch = loss(ls_batch, ys_batch) # CrossEntropyLoss quiere logits\n",
        "    a_batch = accuracy(ps_batch, ys_batch)\n",
        "    optimizer.zero_grad()\n",
        "    e_batch.backward()\n",
        "    optimizer.step()\n",
        "    e_train += e_batch.detach()/len(loader_train) # error promedio por época\n",
        "    a_train += a_batch.detach()/len(loader_train) # accuracy promedio por época\n",
        "\n",
        "  # Vemos si le fue bien en validación\n",
        "  #\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    ls_val = model(xs_val)\n",
        "    ps_val = probability(ls_val) # De logit a probabilidades\n",
        "    e_val = loss(ls_val, ys_val) # CrossEntropyLoss quiere logits\n",
        "    a_val = accuracy(ps_val, ys_val)\n",
        "    if e_val < e_better: # Graba la mejor epoch\n",
        "      i_better = i\n",
        "      e_better = e_val\n",
        "      a_better = a_val\n",
        "      state_better = model.state_dict()\n",
        "\n",
        "  # Reporte\n",
        "  #\n",
        "  print(\n",
        "    'i: '+str(i),\n",
        "    'e_train: {:.5f}'.format(float(e_train)/0.693)+' bits',\n",
        "    'a_train: {:.1f}'.format(100.0*float(a_train))+' %',\n",
        "    'e_val: {:.5f}'.format(float(e_val)/0.693)+' bits',\n",
        "    'a_val: {:.1f}'.format(100.0*float(a_val))+' %',\n",
        "    sep='\\t', flush=True\n",
        "  )\n",
        "\n",
        "model.eval()\n",
        "model.load_state_dict(state_better)\n",
        "with torch.no_grad():\n",
        "  ls_test = model(xs_test)\n",
        "  ps_test = probability(ls_test) # De logit a probabilidades\n",
        "  e_test = loss(ls_test, ys_test) # CrossEntropyLoss quiere logits\n",
        "  a_test = accuracy(ps_test, ys_test)\n",
        "\n",
        "print(\n",
        "  'e_test: {:.5f}'.format(float(e_test)/0.693)+' bits',\n",
        "  'a_test: {:.1f}'.format(100.0*float(a_test))+' %',\n",
        "  sep='\\t', flush=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSy_OrCus7sk",
        "outputId": "b5ccee06-3e6d-4af3-94b2-636f32d1bbd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 341kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.34MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.62MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/datasets/mnist.py:66: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/datasets/mnist.py:71: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i: 0\te_train: 2.99019 bits\ta_train: 20.4 %\te_val: 2.98537 bits\ta_val: 21.7 %\n",
            "i: 1\te_train: 2.23472 bits\ta_train: 42.0 %\te_val: 0.59076 bits\ta_val: 88.6 %\n",
            "i: 2\te_train: 0.52510 bits\ta_train: 87.5 %\te_val: 0.50759 bits\ta_val: 90.3 %\n",
            "i: 3\te_train: 0.46751 bits\ta_train: 88.8 %\te_val: 0.47586 bits\ta_val: 91.2 %\n",
            "i: 4\te_train: 0.44340 bits\ta_train: 89.3 %\te_val: 0.46530 bits\ta_val: 91.5 %\n",
            "i: 5\te_train: 0.43390 bits\ta_train: 89.7 %\te_val: 0.45716 bits\ta_val: 91.3 %\n",
            "i: 6\te_train: 0.43049 bits\ta_train: 89.9 %\te_val: 0.46653 bits\ta_val: 91.1 %\n",
            "i: 7\te_train: 0.40598 bits\ta_train: 90.3 %\te_val: 0.45990 bits\ta_val: 91.3 %\n",
            "i: 8\te_train: 0.40517 bits\ta_train: 90.4 %\te_val: 0.47208 bits\ta_val: 91.2 %\n",
            "i: 9\te_train: 0.39831 bits\ta_train: 90.5 %\te_val: 0.44770 bits\ta_val: 91.7 %\n",
            "i: 10\te_train: 0.38581 bits\ta_train: 90.9 %\te_val: 0.46154 bits\ta_val: 91.5 %\n",
            "i: 11\te_train: 0.38053 bits\ta_train: 90.9 %\te_val: 0.46032 bits\ta_val: 91.5 %\n",
            "i: 12\te_train: 0.39180 bits\ta_train: 90.8 %\te_val: 0.45194 bits\ta_val: 91.7 %\n",
            "i: 13\te_train: 0.38773 bits\ta_train: 90.6 %\te_val: 0.45944 bits\ta_val: 91.5 %\n",
            "i: 14\te_train: 0.38699 bits\ta_train: 90.8 %\te_val: 0.45664 bits\ta_val: 91.6 %\n",
            "i: 15\te_train: 0.38638 bits\ta_train: 90.7 %\te_val: 0.44938 bits\ta_val: 91.7 %\n",
            "i: 16\te_train: 0.36494 bits\ta_train: 91.2 %\te_val: 0.45508 bits\ta_val: 91.7 %\n",
            "i: 17\te_train: 0.37518 bits\ta_train: 91.2 %\te_val: 0.44208 bits\ta_val: 91.5 %\n",
            "i: 18\te_train: 0.36663 bits\ta_train: 91.1 %\te_val: 0.44820 bits\ta_val: 91.8 %\n",
            "i: 19\te_train: 0.36445 bits\ta_train: 91.2 %\te_val: 0.44741 bits\ta_val: 91.8 %\n",
            "i: 20\te_train: 0.35773 bits\ta_train: 91.3 %\te_val: 0.44081 bits\ta_val: 92.0 %\n",
            "i: 21\te_train: 0.36091 bits\ta_train: 91.3 %\te_val: 0.44808 bits\ta_val: 91.7 %\n",
            "i: 22\te_train: 0.36048 bits\ta_train: 91.3 %\te_val: 0.45212 bits\ta_val: 91.8 %\n",
            "i: 23\te_train: 0.37277 bits\ta_train: 91.1 %\te_val: 0.44510 bits\ta_val: 91.5 %\n",
            "i: 24\te_train: 0.36553 bits\ta_train: 91.0 %\te_val: 0.45688 bits\ta_val: 91.2 %\n",
            "i: 25\te_train: 0.35818 bits\ta_train: 91.3 %\te_val: 0.44802 bits\ta_val: 91.8 %\n",
            "i: 26\te_train: 0.34445 bits\ta_train: 91.8 %\te_val: 0.44510 bits\ta_val: 91.9 %\n",
            "i: 27\te_train: 0.36005 bits\ta_train: 91.2 %\te_val: 0.44442 bits\ta_val: 91.7 %\n",
            "i: 28\te_train: 0.35404 bits\ta_train: 91.3 %\te_val: 0.44547 bits\ta_val: 91.8 %\n",
            "i: 29\te_train: 0.35240 bits\ta_train: 91.4 %\te_val: 0.44775 bits\ta_val: 91.7 %\n",
            "i: 30\te_train: 0.34714 bits\ta_train: 91.6 %\te_val: 0.44573 bits\ta_val: 91.8 %\n",
            "i: 31\te_train: 0.34627 bits\ta_train: 91.4 %\te_val: 0.44919 bits\ta_val: 91.7 %\n",
            "i: 32\te_train: 0.34799 bits\ta_train: 91.7 %\te_val: 0.44431 bits\ta_val: 91.7 %\n",
            "i: 33\te_train: 0.34989 bits\ta_train: 91.3 %\te_val: 0.44424 bits\ta_val: 91.8 %\n",
            "i: 34\te_train: 0.34437 bits\ta_train: 91.5 %\te_val: 0.44989 bits\ta_val: 91.6 %\n",
            "i: 35\te_train: 0.34384 bits\ta_train: 91.4 %\te_val: 0.44763 bits\ta_val: 91.8 %\n"
          ]
        }
      ]
    }
  ]
}